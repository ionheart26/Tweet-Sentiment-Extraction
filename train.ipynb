{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NKJuZXS2mbB2","executionInfo":{"status":"ok","timestamp":1614146431319,"user_tz":-540,"elapsed":17238,"user":{"displayName":"Beanstalk Giant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgjQW8P0anoxk4LoRCSxaOtj4FgzygkRISG0azTDQ=s64","userId":"04389221502269427900"}},"outputId":"94c3e28d-5b00-46cd-e588-92748aa71345"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9CdQj7uUxBVc"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZpAeiRuxMKn","executionInfo":{"status":"ok","timestamp":1614146455114,"user_tz":-540,"elapsed":41019,"user":{"displayName":"Beanstalk Giant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgjQW8P0anoxk4LoRCSxaOtj4FgzygkRISG0azTDQ=s64","userId":"04389221502269427900"}},"outputId":"7405afe6-be67-4bb8-f506-955c9a26040e"},"source":["import os\r\n","from transformers import (\r\n","    BertTokenizer,\r\n","    TFBertForSequenceClassification,\r\n",")\r\n","\r\n","model_path = ''\r\n","tokenizer_path = ''\r\n","\r\n","if not os.path.exists(model_path):\r\n","    model =  TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 3)\r\n","    model.save_pretrained(model_path)\r\n","else:\r\n","    model = TFBertForSequenceClassification.from_pretrained(model_path)\r\n","    \r\n","if not os.path.exists(tokenizer_path):\r\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n","    tokenizer.save_pretrained(tokenizer_path)\r\n","else:\r\n","    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)        "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at /content/drive/MyDrive/2020 WINTER PROJ/saved/model/pre-trained were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n","- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/2020 WINTER PROJ/saved/model/pre-trained.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFTReknRxMbi","executionInfo":{"status":"ok","timestamp":1614146455116,"user_tz":-540,"elapsed":41014,"user":{"displayName":"Beanstalk Giant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgjQW8P0anoxk4LoRCSxaOtj4FgzygkRISG0azTDQ=s64","userId":"04389221502269427900"}},"outputId":"4250b836-e98e-4306-900a-a08f498148dc"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bert (TFBertMainLayer)       multiple                  109482240 \n","_________________________________________________________________\n","dropout_37 (Dropout)         multiple                  0         \n","_________________________________________________________________\n","classifier (Dense)           multiple                  2307      \n","=================================================================\n","Total params: 109,484,547\n","Trainable params: 109,484,547\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fym45x4UxZ3c"},"source":["import tensorflow as tf\r\n","import pandas as pd\r\n","from sklearn.model_selection import train_test_split\r\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213},"id":"C4TSKhwhxuix","executionInfo":{"status":"ok","timestamp":1614146456709,"user_tz":-540,"elapsed":42593,"user":{"displayName":"Beanstalk Giant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgjQW8P0anoxk4LoRCSxaOtj4FgzygkRISG0azTDQ=s64","userId":"04389221502269427900"}},"outputId":"41d7eae0-667d-42ac-fa2a-51252621e556"},"source":["train = pd.read_csv('')\r\n","train = train.dropna()\r\n","print('Train data shape: ', train.shape)\r\n","train.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train data shape:  (27480, 4)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>textID</th>\n","      <th>text</th>\n","      <th>selected_text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cb774db0d1</td>\n","      <td>I`d have responded, if I were going</td>\n","      <td>I`d have responded, if I were going</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>549e992a42</td>\n","      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n","      <td>Sooo SAD</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>088c60f138</td>\n","      <td>my boss is bullying me...</td>\n","      <td>bullying me</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9642c003ef</td>\n","      <td>what interview! leave me alone</td>\n","      <td>leave me alone</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>358bd9e861</td>\n","      <td>Sons of ****, why couldn`t they put them on t...</td>\n","      <td>Sons of ****,</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       textID  ... sentiment\n","0  cb774db0d1  ...   neutral\n","1  549e992a42  ...  negative\n","2  088c60f138  ...  negative\n","3  9642c003ef  ...  negative\n","4  358bd9e861  ...  negative\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"2kP3mZAM9BeR"},"source":["def to_index(text):\r\n","    return {\r\n","        'negative': 0, 'neutral': 1, 'positive': 2\r\n","    }[text]\r\n","\r\n","train['sentiment_i'] = train.sentiment.apply(to_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1J8TbGR5POu"},"source":["def to_train_dataset(data, DATA_COLUMN, LABEL_COLUMN, tokenizer, max_length=128):\r\n","  '''\r\n","  convert df into tf dataset\r\n","  '''\r\n","  # dict of 2d lists\r\n","  tokenized = tokenizer(\r\n","      list(data[DATA_COLUMN]),\r\n","      add_special_tokens=True,\r\n","      max_length=max_length,\r\n","      return_token_type_ids=True,\r\n","      return_attention_mask=True,\r\n","      pad_to_max_length=True,\r\n","      truncation=True,\r\n","  )\r\n","\r\n","  dicts = [{\r\n","    'input_ids': input_ids,\r\n","    'attention_mask': attention_mask,\r\n","    'token_type_ids': token_type_ids,\r\n","    } for input_ids, attention_mask, token_type_ids in zip(tokenized['input_ids'], tokenized['attention_mask'], tokenized['token_type_ids'])]\r\n","    \r\n","  def gen():\r\n","      for d, label in zip(dicts, data[LABEL_COLUMN]):\r\n","          yield (d, label)\r\n","\r\n","  return tf.data.Dataset.from_generator(\r\n","      gen,\r\n","      ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'token_type_ids': tf.int32}, tf.int32),\r\n","      (\r\n","          {\r\n","              'input_ids': tf.TensorShape([None]),\r\n","              'attention_mask': tf.TensorShape([None]),\r\n","              'token_type_ids': tf.TensorShape([None]),\r\n","          },\r\n","          tf.TensorShape([]),\r\n","      ),\r\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wZNs-ZQ5r7E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614146466032,"user_tz":-540,"elapsed":51898,"user":{"displayName":"Beanstalk Giant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgjQW8P0anoxk4LoRCSxaOtj4FgzygkRISG0azTDQ=s64","userId":"04389221502269427900"}},"outputId":"fbd9ea4c-85ba-4d35-f699-ecc3c2b78db4"},"source":["DATA_COLUMN = 'text'\r\n","LABEL_COLUMN = 'sentiment_i'\r\n","\r\n","train, validation = train_test_split(train, test_size = 0.2)\r\n","\r\n","train_data = to_train_dataset(train, DATA_COLUMN, LABEL_COLUMN, tokenizer)\r\n","train_data = train_data.shuffle(100).batch(32).repeat(2)\r\n","\r\n","validation_data =to_train_dataset(validation, DATA_COLUMN, LABEL_COLUMN, tokenizer)\r\n","validation_data = validation_data.shuffle(100).batch(32).repeat(2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8c1zq5J6gnd","executionInfo":{"status":"ok","timestamp":1614148859764,"user_tz":-540,"elapsed":2445620,"user":{"displayName":"Beanstalk Giant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgjQW8P0anoxk4LoRCSxaOtj4FgzygkRISG0azTDQ=s64","userId":"04389221502269427900"}},"outputId":"65ed52fe-ae56-464a-821b-2be14f328af2"},"source":["model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\r\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n","              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\r\n","\r\n","model.fit(train_data, epochs=2, validation_data=validation_data)\r\n","\r\n","model.save_pretrained('')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f54b213fbb0>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f54b213fbb0>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f54cd4ed0e0> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <function wrap at 0x7f54cd4ed0e0> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","   1374/Unknown - 1126s 784ms/step - loss: 0.6237 - accuracy: 0.7311WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","1374/1374 [==============================] - 1217s 850ms/step - loss: 0.6236 - accuracy: 0.7312 - val_loss: 0.5842 - val_accuracy: 0.7789\n","Epoch 2/2\n","1374/1374 [==============================] - 1168s 850ms/step - loss: 0.2663 - accuracy: 0.9005 - val_loss: 0.8040 - val_accuracy: 0.7682\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"corMLVtmkngG"},"source":["pred_sentences = ['This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good',\r\n","                  'One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie',\r\n","                  'This book is about operating system concepts',\r\n","                  \"I played Cyberpunk 2077 last night.\",\r\n","                  'There was an accident last night...',\r\n","                  'There was an accident last night... so sad...']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3GPvTSAHO3f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610443078091,"user_tz":-540,"elapsed":982,"user":{"displayName":"Jim Jeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgxMUONNpuLtHjFVrW4mCw_-p090hrh3ZFcFy9GTQ=s64","userId":"12337624229734791070"}},"outputId":"1751bfc9-996f-4dbc-8de9-16d1b064e400"},"source":["tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\r\n","tf_outputs = model(tf_batch)\r\n","tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\r\n","labels = ['Negative', 'Neutral', 'Positive']\r\n","label = tf.argmax(tf_predictions, axis=1)\r\n","label = label.numpy()\r\n","for i in range(len(pred_sentences)):\r\n","  print(pred_sentences[i], \": \\n\", labels[label[i]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good : \n"," Positive\n","One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie : \n"," Negative\n","This book is about operating system concepts : \n"," Neutral\n","I played Cyberpunk 2077 last night. : \n"," Neutral\n","There was an accident last night... : \n"," Negative\n","There was an accident last night... so sad... : \n"," Negative\n"],"name":"stdout"}]}]}